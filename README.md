# End-to-End-Big-Data-Engineering-Pipeline-Project
An end-to-end big data engineering pipeline built on AWS, leveraging EMR and Apache Spark for large-scale data processing, S3 for scalable storage, and Redshift Serverless for analytics-ready data validation and querying.
# Architecture Flow
Local System → S3 → EMR (Spark) → S3(Transformed data) → Redshift Serverless
# Step 1: EMR Cluster Creation
<img width="1893" height="807" alt="EMR project(1)" src="https://github.com/user-attachments/assets/a091d2df-3176-4fa0-9538-3400732a9eda" />


# Step 2: EMR Remote Development & Cluster Access Configuration
Configured secure remote access to the EMR cluster for Spark application development and execution.
<img width="1918" height="987" alt="EMR project(2)" src="https://github.com/user-attachments/assets/afd68a96-3369-4d8e-9237-96aa57c3386f" />
<img width="1918" height="1017" alt="EMR project(3)" src="https://github.com/user-attachments/assets/855fda0b-c83a-4e08-ada5-f6ef93a22076" />


# Step 3: GitHub Archive Data Ingestion
Downloaded GitHub Archive event data for large-scale analytical processing.
<img width="1920" height="1080" alt="EMR(12)" src="https://github.com/user-attachments/assets/04c75a7a-f1e6-4185-b0d2-da926409793b" />




